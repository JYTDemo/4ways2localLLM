{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87daf85-41e5-47ff-9b06-dd6e4f0f3928",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b93223-eeaa-4752-ae5f-85cbad02ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791a46a-bea4-4161-b125-9a85ff446f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\" \n",
    "FILENAME = \"mistral-7b-instruct-v0.2.Q3_K_S.gguf\"\n",
    "local_dir='C:/Users/67997/.cache/huggingface/hub/models'\n",
    "model_path = f'{local_dir}/{FILENAME}'\n",
    "check_file = os.path.isfile(model_path)\n",
    "if check_file == False :\n",
    "    model_path = hf_hub_download(repo_id=REPO_ID, \n",
    "                                filename=FILENAME,\n",
    "                                local_dir=local_dir,\n",
    "                                local_dir_use_symlinks=False\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e056c-ddb7-4343-9f13-0066f7baa3f1",
   "metadata": {},
   "source": [
    "# CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734143e-402c-4bd9-afbd-942a1589728a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "035ba44b-91d3-4d2e-8430-32bea9346b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:14:10.721298Z",
     "iopub.status.busy": "2024-01-28T13:14:10.721298Z",
     "iopub.status.idle": "2024-01-28T13:14:12.690025Z",
     "shell.execute_reply": "2024-01-28T13:14:12.689528Z",
     "shell.execute_reply.started": "2024-01-28T13:14:10.721298Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Any, Dict, List, Union\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3942bb8a-33a4-40c5-bb8c-00c8517fb5db",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-01-28T13:14:31.801841Z",
     "iopub.status.busy": "2024-01-28T13:14:31.801587Z",
     "iopub.status.idle": "2024-01-28T13:14:35.337140Z",
     "shell.execute_reply": "2024-01-28T13:14:35.336557Z",
     "shell.execute_reply.started": "2024-01-28T13:14:31.801841Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9acf4dbb77548f08c6b322b6efda33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b73ba5338948deb37db7f1a4f45ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    model_id = 'TheBloke/Mistral-7B-codealpaca-lora-GGUF'\n",
    "    config = {'temperature':0.00, 'context_length':4000,} \n",
    "    llm = CTransformers(model=model_id, \n",
    "                model_type='mistral',\n",
    "                config=config,\n",
    "                streaming=True,\n",
    "                callbacks=[StreamingStdOutCallbackHandler()]\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d02cf37-a635-4fdf-ac58-197f648236de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:14:40.417899Z",
     "iopub.status.busy": "2024-01-28T13:14:40.417899Z",
     "iopub.status.idle": "2024-01-28T13:14:40.421825Z",
     "shell.execute_reply": "2024-01-28T13:14:40.421825Z",
     "shell.execute_reply.started": "2024-01-28T13:14:40.417899Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"you are an assistant answer the following : {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae73a016-72c2-4040-805f-d1f1b3d1b4f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:14:43.640723Z",
     "iopub.status.busy": "2024-01-28T13:14:43.640331Z",
     "iopub.status.idle": "2024-01-28T13:14:43.644260Z",
     "shell.execute_reply": "2024-01-28T13:14:43.644260Z",
     "shell.execute_reply.started": "2024-01-28T13:14:43.640723Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3ee2c2b-5c70-4fc9-9f30-0b1bfc2a68c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:14:46.947062Z",
     "iopub.status.busy": "2024-01-28T13:14:46.946033Z",
     "iopub.status.idle": "2024-01-28T13:15:52.282887Z",
     "shell.execute_reply": "2024-01-28T13:15:52.281669Z",
     "shell.execute_reply.started": "2024-01-28T13:14:46.947062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and what is his iq?\n",
      "\n",
      "Sherlock Holmes is a fictional detective created by Arthur Conan Doyle. He is known for his exceptional intelligence, observation skills, and ability to solve complex cases. Sherlock Holmes is considered to be extremely intelligent with an eidetic memory, which allows him to recall details of every case he has ever worked on.\n",
      "\n",
      "As for his IQ, there isn't a specific number associated with Sherlock Holmes in the books written by Arthur Conan Doyle. However, many fans and interpreters of the character have estimated his IQ to be exceptionally high, reflecting his exceptional intelligence and deductive reasoning abilities. Some have even guessed that his IQ could range from 180 to 200, which is within the top 1% intellectual range. and what is his iq?\n",
      "\n",
      "Sherlock Holmes is a fictional detective created by Arthur Conan Doyle. He is known for his exceptional intelligence, observation skills, and ability to solve complex cases. Sherlock Holmes is considered to be extremely intelligent with an eidetic memory, which allows him to recall details of every case he has ever worked on.\n",
      "\n",
      "As for his IQ, there isn't a specific number associated with Sherlock Holmes in the books written by Arthur Conan Doyle. However, many fans and interpreters of the character have estimated his IQ to be exceptionally high, reflecting his exceptional intelligence and deductive reasoning abilities. Some have even guessed that his IQ could range from 180 to 200, which is within the top 1% intellectual range.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"who is sherlok holmes\")\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37200b-5789-4d9d-9400-560828e1ce0a",
   "metadata": {},
   "source": [
    "# CTansformers Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de205a88-d7d6-431a-9adc-22798056e2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:18:52.012928Z",
     "iopub.status.busy": "2024-02-10T14:18:52.011930Z",
     "iopub.status.idle": "2024-02-10T14:18:52.909342Z",
     "shell.execute_reply": "2024-02-10T14:18:52.908335Z",
     "shell.execute_reply.started": "2024-02-10T14:18:52.012928Z"
    }
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c71007-57c2-4128-a1c1-6f82b518caf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:18:55.662952Z",
     "iopub.status.busy": "2024-02-10T14:18:55.661955Z",
     "iopub.status.idle": "2024-02-10T14:19:10.331259Z",
     "shell.execute_reply": "2024-02-10T14:19:10.330258Z",
     "shell.execute_reply.started": "2024-02-10T14:18:55.662952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a508d359ab3642aa8d6abdb4011c1df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"TheBloke/MPT-7B-GGML\", \n",
    "                                    context_length=8192,\n",
    "                                    max_new_tokens=512)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "        './model/mpt-7b.ggmlv3.q4_0.bin',\n",
    "        model_type=\"mpt\",\n",
    "        config=config,\n",
    "        temparature=0.00\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53757496-3ba5-4188-9cca-cbb350caed57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:19:16.475384Z",
     "iopub.status.busy": "2024-02-10T14:19:16.474526Z",
     "iopub.status.idle": "2024-02-10T14:22:58.439068Z",
     "shell.execute_reply": "2024-02-10T14:22:58.438134Z",
     "shell.execute_reply.started": "2024-02-10T14:19:16.475384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Charlie Chaplin was born on April 16, 1889 in Lambeth, London. He had a total of five children with his wife Oona O\u0019Neil and he adopted two more kids when he married actress Lita Grey during the 1930s.. Charlie Chapinn would have been 130 years old today! They got hitched after meeting at an audition for Chaplin's 1915 movie 'The Kid'. The film was called \u001c",
      "Charlie & He died on Christmas Eve. Biography of comedian, actor and director Charley Chase who worked in vaudeville with the Keystone Kops before joining Charlie Chaplin as one of his most popular comedians during his \"Silent Film Era\" years from 1916-1922.; See also: Vaudevillian Actors, Screen Comedies. He was born on 16 April 1889, at 10 Courtland Street in Lambeth district and died after falling down the stairs while drunk Christmas Eve 1974 when he suffered a broken neck.. The film is one of Chaplin\u0019s earliest comedic shorts from 1918 featuring his signature Tramp character with some classic scenes such as him trying to get & In this short, Charlie plays an actor who gets fired by another actor because there was too many acting going on. He died in 1974 at the age of 85 after falling down a flight of stairs while drunk and suffering severe head injuries.. The film is one of Chaplin\u0019s earliest comedic shorts from 1918 featuring his signature Tramp character with some classic scenes such as him trying to get & In this short, Charlie plays an actor who gets fired by another actor because there was too much acting going on. He died in 1974 at the age of 85 after falling down a flight of stairs while drunk and suffering severe head injuries.. The film is one of Chaplin\u0019s earliest comedic shorts from 1918 featuring his signature Tramp character with some classic scenes such as him trying to get & Charlie was born into an upper class, wealthy family. His father Frederic Chaplin who he would later take the name Charles after died when Charley and brother Sydney were very young (8 months old). He is one of a few people in history that are remembered by just their first names like Michael Jackson or Marilyn Monroe etc.. The film was called \u001c",
      "Charlie Chaplin\u001d",
      " & Who\u0019s next? Charlie chaplins daughter. After being rejected many times from studios he worked at, after some time his work began to catch on and now it is one of the most popular directors that"
     ]
    }
   ],
   "source": [
    "for text in llm(\"who is charlie chaplin\",stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35595f01-ff23-4311-91e5-66ee6923de91",
   "metadata": {},
   "source": [
    "# LLAMA CPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46e6cb-6689-4033-ad10-6877b45dc1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7496e1db-9478-489a-a0f7-21b2e798fb7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:19:04.787737Z",
     "iopub.status.busy": "2024-01-28T13:19:04.787323Z",
     "iopub.status.idle": "2024-01-28T13:19:04.790795Z",
     "shell.execute_reply": "2024-01-28T13:19:04.790795Z",
     "shell.execute_reply.started": "2024-01-28T13:19:04.787737Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ba43454-4ce9-4e36-8bb8-d361044d5455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:18:45.289023Z",
     "iopub.status.busy": "2024-01-28T13:18:45.288051Z",
     "iopub.status.idle": "2024-01-28T13:18:45.293440Z",
     "shell.execute_reply": "2024-01-28T13:18:45.292437Z",
     "shell.execute_reply.started": "2024-01-28T13:18:45.289023Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path='C:/Users/67997/.cache/huggingface/hub/models/mistral-7b-instruct-v0.2.Q3_K_S.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e246f173-7b17-418a-abfa-2a9c73f49d38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:19:10.419938Z",
     "iopub.status.busy": "2024-01-28T13:19:10.418941Z",
     "iopub.status.idle": "2024-01-28T13:19:10.422805Z",
     "shell.execute_reply": "2024-01-28T13:19:10.422805Z",
     "shell.execute_reply.started": "2024-01-28T13:19:10.418941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c50eac32-db3e-4070-83b9-0fd9877839e8",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-01-28T13:19:13.047238Z",
     "iopub.status.busy": "2024-01-28T13:19:13.046077Z",
     "iopub.status.idle": "2024-01-28T13:19:13.735075Z",
     "shell.execute_reply": "2024-01-28T13:19:13.734825Z",
     "shell.execute_reply.started": "2024-01-28T13:19:13.047238Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\01MyFolder\\mypython\\python-3.11.5-embed-amd64\\repository\\jpynb\\Lib\\site-packages\\langchain_core\\utils\\utils.py:159: UserWarning: WARNING! context_length is not default parameter.\n",
      "                context_length was transferred to model_kwargs.\n",
      "                Please confirm that context_length is what you intended.\n",
      "  warnings.warn(\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '11', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.00,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=2048,\n",
    "    context_length=4000,\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1488fca1-39bc-4004-bab8-efeb52847fc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:19:18.911909Z",
     "iopub.status.busy": "2024-01-28T13:19:18.910906Z",
     "iopub.status.idle": "2024-01-28T13:19:18.915412Z",
     "shell.execute_reply": "2024-01-28T13:19:18.915412Z",
     "shell.execute_reply.started": "2024-01-28T13:19:18.911909Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"you are an assistant answer the following : {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "def11da4-bce2-4c4b-b24e-039c9bf7a7b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:19:20.206212Z",
     "iopub.status.busy": "2024-01-28T13:19:20.205058Z",
     "iopub.status.idle": "2024-01-28T13:19:20.459171Z",
     "shell.execute_reply": "2024-01-28T13:19:20.458780Z",
     "shell.execute_reply.started": "2024-01-28T13:19:20.206212Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acc1003e-065d-4485-a56d-b9ee871c67f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:19:25.311893Z",
     "iopub.status.busy": "2024-01-28T13:19:25.309625Z",
     "iopub.status.idle": "2024-01-28T13:20:10.716490Z",
     "shell.execute_reply": "2024-01-28T13:20:10.714433Z",
     "shell.execute_reply.started": "2024-01-28T13:19:25.311893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sherlock Holmes is a fictional detective created by British author Sir Arthur Conan Doyle. He first appeared in print in 1887, and has since become one of the most famous and enduring figures in literature.\n",
      "\n",
      "Holmes is known for his logical reasoning, astute observation, and ability to disguise himself. He often works with his friend and confidant, Dr. John Watson. Together they solve a series of intriguing and often dangerous cases.\n",
      "\n",
      "Sherlock Holmes is a fictional detective created by British author Sir Arthur Conan Doyle. He first appeared in print in 1887, and has since become one of the most famous and enduring figures in literature.\n",
      "\n",
      "Holmes is known for his logical reasoning, astute observation, and ability to disguise himself. He often works with his friend and confidant, Dr. John Watson. Together they solve a series of intriguing and often dangerous cases.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"who is sherlok holmes\")\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e336d-2cb5-473e-bd95-f7ff85fa8f39",
   "metadata": {},
   "source": [
    "# Transformer Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b717f4-dd59-4fc0-ad18-b2c45f78e8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8c5976-24da-4359-b24d-8ff7ede3bb61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:23:43.175792Z",
     "iopub.status.busy": "2024-02-10T14:23:43.175792Z",
     "iopub.status.idle": "2024-02-10T14:23:52.486195Z",
     "shell.execute_reply": "2024-02-10T14:23:52.485703Z",
     "shell.execute_reply.started": "2024-02-10T14:23:43.175792Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig, pipeline,AutoTokenizer,TextStreamer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0cea44-26a7-41d5-954f-90f1f369124e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:23:55.119202Z",
     "iopub.status.busy": "2024-02-10T14:23:55.118205Z",
     "iopub.status.idle": "2024-02-10T14:24:00.764830Z",
     "shell.execute_reply": "2024-02-10T14:24:00.764830Z",
     "shell.execute_reply.started": "2024-02-10T14:23:55.118205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c98af5f6aa4a108fd583fcc3469e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\01MyFolder\\mypython\\python-3.11.5-embed-amd64\\repository\\jpynb\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\67997\\.cache\\huggingface\\hub\\models--stabilityai--stable-code-3b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e028726b11a4651920d499739442700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248224e1c0d0439fbf325d6f5d897c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"stabilityai/stable-code-3b\" #\"microsoft/phi-2\"\n",
    "model_type = \"text-generation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0adf5f-d0ed-4ef3-89b5-8f42df07b49b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:24:08.872465Z",
     "iopub.status.busy": "2024-02-10T14:24:08.871466Z",
     "iopub.status.idle": "2024-02-10T14:35:14.468109Z",
     "shell.execute_reply": "2024-02-10T14:35:14.467633Z",
     "shell.execute_reply.started": "2024-02-10T14:24:08.872465Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809ab5dd1fdc4f95bfe96a5e1fb18c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/815 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff14b8e3ef94e679defc13ecfc9c948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_stablelm_epoch.py:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/stabilityai/stable-code-3b:\n",
      "- configuration_stablelm_epoch.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b76e5946e0f42ea8988e9da69e4a0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_stablelm_epoch.py:   0%|          | 0.00/38.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/stabilityai/stable-code-3b:\n",
      "- modeling_stablelm_epoch.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf37780c97b44b24a42d0c3262eaec6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/29.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91564bbe8a14992b9be44f400bd599f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ed41a04784fe4ac82a7481264bd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c002d02230e4addac048033f3609964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/610M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb0dc7c3ee64b03acf11355547057de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdf6911f9424a908fda149d02f3e38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf6bab-1544-4865-9d80-0c912581cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters that can be used on a system with GPU\n",
    "    #load_in_8bit=True,\n",
    "    #device_map=\"cpu\",\n",
    "    #torch_dtype=torch.float16,\n",
    "    #low_cpu_mem_usage=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d536ed3-294d-4a13-aac4-8a98ccd446f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:35:21.932905Z",
     "iopub.status.busy": "2024-02-10T14:35:21.930910Z",
     "iopub.status.idle": "2024-02-10T14:35:21.939774Z",
     "shell.execute_reply": "2024-02-10T14:35:21.939591Z",
     "shell.execute_reply.started": "2024-02-10T14:35:21.932905Z"
    }
   },
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251a7d58-e266-4912-b0cb-3d38487f857b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:35:25.559461Z",
     "iopub.status.busy": "2024-02-10T14:35:25.558464Z",
     "iopub.status.idle": "2024-02-10T14:35:25.603952Z",
     "shell.execute_reply": "2024-02-10T14:35:25.603446Z",
     "shell.execute_reply.started": "2024-02-10T14:35:25.559461Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    model_type,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 1024,\n",
    "    temperature=0,\n",
    "    #top_p=0.95,\n",
    "    #repetation_penalty = 1.15,\n",
    "    streamer=streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b28957-1185-44eb-b421-6f6167f221ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:35:30.836765Z",
     "iopub.status.busy": "2024-02-10T14:35:30.835911Z",
     "iopub.status.idle": "2024-02-10T14:35:30.866246Z",
     "shell.execute_reply": "2024-02-10T14:35:30.864560Z",
     "shell.execute_reply.started": "2024-02-10T14:35:30.836765Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c79ee4-7ff7-46d0-bb95-bca5bf27126b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:37:54.687772Z",
     "iopub.status.busy": "2024-02-10T14:37:54.687518Z",
     "iopub.status.idle": "2024-02-10T14:37:54.693106Z",
     "shell.execute_reply": "2024-02-10T14:37:54.692102Z",
     "shell.execute_reply.started": "2024-02-10T14:37:54.687772Z"
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"As a python coder answer the user question pythonic way.\n",
    "Question: {question}\n",
    "Assisstant:  \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266797c5-5601-4af1-9d98-17ef3e969db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T14:36:06.508697Z",
     "iopub.status.busy": "2024-02-10T14:36:06.507877Z",
     "iopub.status.idle": "2024-02-10T14:37:54.656946Z",
     "shell.execute_reply": "2024-02-10T14:37:54.655867Z",
     "shell.execute_reply.started": "2024-02-10T14:36:06.508697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\01MyFolder\\mypython\\python-3.11.5-embed-amd64\\repository\\jpynb\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a python coder answer the user question pythonic way.\n",
      "Question: write a python code to insert a - after the third charecter in df['column_name']\n",
      "Assisstant: Answer \n",
      "'''\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.DataFrame({'column_name': ['abc', 'def', 'ghi', 'jkl', mno']})\n",
      "\n",
      "df['column_name'] = df['column_name'].str[0:3] + '-' + df['column_name'].str[3:]\n",
      "\n",
      "print(df)\n",
      "\n",
      "'''\n",
      "Output:\n",
      "    column_name\n",
      "0         abc-\n",
      "1         def-\n",
      "2         ghi-\n",
      "3         jkl-\n",
      "4         mno-\n",
      "'''\n",
      "<|endoftext|>\n",
      "\n",
      "'''\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.DataFrame({'column_name': ['abc', 'def', 'ghi', 'jkl','mno']})\n",
      "\n",
      "df['column_name'] = df['column_name'].str[0:3] + '-' + df['column_name'].str[3:]\n",
      "\n",
      "print(df)\n",
      "\n",
      "'''\n",
      "Output:\n",
      "    column_name\n",
      "0         abc-\n",
      "1         def-\n",
      "2         ghi-\n",
      "3         jkl-\n",
      "4         mno-\n",
      "'''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"write a python code to insert a - after the third charecter in df['column_name']\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28756465-65a7-4210-9b1d-b0d1c07aa471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
